{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter User Recommeder\n",
    "\n",
    "This notebook details the approaches and the conclusions of building a twitter recommender.\n",
    "\n",
    "<b>This notebook covers:</b>\n",
    "1. Data Acquisition\n",
    "2. Data Cleaning & Formatting\n",
    "3. Training and Optimizing LDA\n",
    "4. Detailing Next Steps\n",
    "\n",
    "<b>Our Approach:</b>\n",
    "1. Our approach was to get twitter data via the Twitter Search API and derive topics mentioned by a user.\n",
    "2. Used Latent Dirichlet allocation derive the topics and optimized it using coherenc.\n",
    "3. I found topics for the main user and 2nd degree users and found the ones with the highest overlaps.\n",
    "4. The users were then ranked according to how many topics they have 2 words in common.\n",
    "\n",
    "<b> Uses:</b>\n",
    "1. This can be used to recommend followers to others based on the topics that folks talk about.\n",
    "2. Via an app one can learn about others that have similar interests.\n",
    "\n",
    "<b> Next Steps:</b>\n",
    "1. Try RMF unsupervised learning\n",
    "2. Optimize it to cut down the time\n",
    "3. Get access to larger amounts of data\n",
    "4. Build An App\n",
    "\n",
    "### Aside from that, I hope you enjoy this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy \n",
    "access_token = \"318256106-lLR709QO7OzkUSQ549ji1bKzuCggcaNBbdDJkq9U\"\n",
    "access_token_secret = \"cm14qVXM45PCsEutAp8a2haKgTAzwClkS55XusLUXlodO\"\n",
    "consumer_key = \"ObXTJ4Qwrt9rnDr1SlbYAF1nu\"\n",
    "consumer_secret = \"oU1BOffC7zOoqXsCuiBXw5ul3vHAjyK2XgyeRF156HG1Z0rbTs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donald_raw = pd.read_csv('Donald-Tweets!.csv')\n",
    "donald_raw.columns = [i.lower() for i in donald_raw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donald = donald_raw[donald_raw.type == 'text']\n",
    "user_tweets = donald.tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "#user = api.get_user('kanyewest')\n",
    "#\n",
    "#\n",
    "### Get Elon's Tweets\n",
    "##stuff = api.user_timeline(screen_name = 'elonmusk', count = 5000, include_rts = False)\n",
    "##user_tweets = [i.text for i in  stuff]\n",
    "#    \n",
    "## Get The People Elon Follows\n",
    "users_friends = [i for i in api.friends_ids(screen_name = 'realDonaldTrump')]\n",
    "\n",
    "\n",
    "##Getting 2nd degree followers\n",
    "second_degree_friends = []\n",
    "for ids in users_friends[:35]:\n",
    "    second_degree_friends.extend([i for i in api.friends_ids(id = ids, count = 20)])\n",
    "\n",
    "   \n",
    " final_second_degree_friends = [i for i in second_degree_friends if i not in set(users_friends)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_second_degree_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_degree_friends_user_names = []\n",
    "\n",
    "for i,friend_ids in enumerate(final_second_degree_friends[:50]):\n",
    "    try:\n",
    "        second_degree_friends_user_names.append(api.get_user(friend_ids).screen_name )\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pt1. Remove Second Degree Connections That Are News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = ('womenfortrump', 'fema', 'flotus', 'potus', 'huffpost', 'secondlady', \n",
    "           'realdonaldtrump', 'VPPressSec', 'axios', 'vppresssec', 'oann', 'noaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_second_degree_screen_name = [i for i in second_degree_friends_user_names if i.lower() not in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting 2nd degree connection tweets    \n",
    "second_degree_tweets = {}\n",
    "for screen_name in final_second_degree_screen_name[-10:]:\n",
    "    try:\n",
    "        stuff = api.user_timeline( screen_name= screen_name, count = 200, include_rts = False)\n",
    "        second_degree_tweets[screen_name] = [i.text for i in  stuff]    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_degree_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_degree_tweets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_degree_tweets.values()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pt2. Stem, Lemmatize and Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokenized_tweets = []\n",
    "    for tweet in user_tweets:\n",
    "        tokenized_tweet = nlp(tweet.decode('unicode-escape'))\n",
    "\n",
    "        tweet = \"\" # we want to keep each tweet seperate\n",
    "\n",
    "        for token in tokenized_tweet:\n",
    "            if token.is_space:\n",
    "                continue\n",
    "            elif token.is_punct:\n",
    "                continue\n",
    "            elif token.is_stop:\n",
    "                continue\n",
    "            elif token.is_digit:\n",
    "                continue\n",
    "            elif len(token) == 1:\n",
    "                continue\n",
    "            elif len(token) == 2:\n",
    "                continue\n",
    "            elif '@' in str(token):\n",
    "                continue\n",
    "\n",
    "            elif 'http' in str(token):\n",
    "                continue\n",
    "            elif 'amp' in str(token):\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    tweet += str(token.lemma_) + \" \" #creating lemmatized version of tweet\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        tokenized_tweets.append(tweet)\n",
    "    tokenized_tweets = list(map(str.strip, tokenized_tweets)) # strip whitespace\n",
    "    tokenized_tweets = [x for x in tokenized_tweets if x != \"\"] # remove empty entries\n",
    "    \n",
    "    gensim_tweets = []\n",
    "    for tweet in tokenized_tweets:\n",
    "        gensim_tweets.append(tweet.replace('-PRON- ', '').replace('the ', '').split(' '))\n",
    "    \n",
    "    return tokenized_tweets, gensim_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dict (tokenized_tweets):\n",
    "    gensim_dict = Dictionary(tokenized_tweets)\n",
    "\n",
    "    #removes really rare words and really frequent words\n",
    "    gensim_dict.filter_extremes(no_below=5, no_above=0.7)\n",
    "\n",
    "    #remove spaces between the  removed words\n",
    "    gensim_dict.compactify() # remove gaps after words that were removed\n",
    "    \n",
    "    return gensim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus (dictionary, tokenized_text):\n",
    "\n",
    "    corpus = [dictionary.doc2bow(i) for i in tokenized_text ]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Model (Pick # Of Topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lda(dictionary, corpus, tokenized_text):\n",
    "    limit=40; start=1; step=3;\n",
    "\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=dictionary, \n",
    "                                                        corpus=corpus,\n",
    "                                                        texts=tokenized_text, start=start,\n",
    "                                                        limit=limit, step=step)\n",
    "    x = range(start, limit, step)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,value in enumerate(coherence_values):\n",
    "        if (value >= max(coherence_values) *.95) & (value <= max(coherence_values) ):\n",
    "            optimal_num = x[i]\n",
    "            break    \n",
    "\n",
    "        \n",
    "    num_topics=optimal_num\n",
    "\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=text_dictionary)\n",
    "    \n",
    "    return lda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics(lda):\n",
    "    temp_dict = {}\n",
    "\n",
    "    for o in range(lda.num_topics):\n",
    "        temp_dict[o] =   [i[0] for i in lda.show_topic(o, topn = 10)]\n",
    "        \n",
    "    return temp_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7. Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Get Main User Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets, tokenized_tweets = clean_text(user_tweets)\n",
    "text_dictionary = create_dict(tokenized_tweets)\n",
    "corpus = create_corpus(text_dictionary, tokenized_tweets)\n",
    "lda = run_lda(text_dictionary, corpus, tokenized_tweets)\n",
    "trump_topics = get_topics(lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print lda.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Get Secondary Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "followers_topic_dict = {}\n",
    "\n",
    "for o,i in enumerate(second_degree_tweets.keys()):\n",
    "\n",
    "    temp_text = second_degree_tweets[i]\n",
    "    \n",
    "    \n",
    "    temp_clean_tweets, temp_tokenized_tweets = clean_text(temp_text)\n",
    "    temp_text_dictionary = create_dict(temp_tokenized_tweets)\n",
    "    temp_corpus = create_corpus(temp_text_dictionary, temp_tokenized_tweets)\n",
    "    temp_lda = run_lda(temp_text_dictionary, temp_corpus, temp_tokenized_tweets)\n",
    "    temp_topics = get_topics(temp_lda)\n",
    "    \n",
    "    followers_topic_dict[i] = temp_topics\n",
    "    print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'thebradfordfile',\n",
       " u'specialkharvey',\n",
       " u'chuckwoolery',\n",
       " u'emilychangtv',\n",
       " u'PARISDENNARD',\n",
       " u'CraigCaplan',\n",
       " u'DineshDSouza',\n",
       " u'reingowsky',\n",
       " u'RealCandaceO',\n",
       " u'paulsperry_']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followers_topic_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Cross Check and Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dict = {}\n",
    "for follower_ in followers_topic_dict.keys():\n",
    "    count = 0 \n",
    "    points_dict[follower_] = 0\n",
    "    for topic_ in followers_topic_dict[follower_].values():\n",
    "        if count < 1:\n",
    "        \n",
    "            for trump_topic in trump_topics.values():\n",
    "\n",
    "\n",
    "                    if len(set(topic_).intersection(trump_topic)) == 2 :\n",
    "                            points_dict[follower_] +=1\n",
    "                            count +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended = []\n",
    "for i in sorted(points_dict.values(), reverse = True)[:3]:\n",
    "    for item in points_dict.items():\n",
    "        if i == item[1]:\n",
    "            recommended.append([item[0], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'reingowsky', 7], [u'CraigCaplan', 6], [u'RealCandaceO', 5]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____\n",
      "reingowsky\n",
      "[u'trump', u'good', u'need', u'know', u'love', u'donaldtrump', u'right', u'reagan', u'time', u'what']\n",
      "[u'like', u'politician', u'talk', u'run', u'trump', u'great', u'silent', u'president', u'tell', u'people']\n",
      "[u'money', u'illegal', u'give', u'win', u'help', u'like', u'need', u'all', u'know', u'trump']\n",
      "[u'thank', u'trump2016', u'great', u'makeamericagreatagain', u'people', u'amazing', u'trump', u'new', u'go', u'lets']\n",
      "[u'jeb', u'bush', u'presidential', u'walker', u'candidate', u'clown', u'trump', u'million', u'thank', u'news']\n",
      "[u'america', u'make', u'great', u'again', u'trump', u'true', u'rubio', u'deal', u'matter', u'beck']\n",
      "[u'border', u'fail', u'read', u'guy', u'sure', u'story', u'old', u'gun', u'change', u'trump']\n",
      "_____\n",
      "CraigCaplan\n",
      "[u'trump', u'good', u'need', u'know', u'love', u'donaldtrump', u'right', u'reagan', u'time', u'what']\n",
      "[u'trump', u'donald', u'makeamericagreatagain', u'love', u'thank', u'vote', u'trump2016', u'people', u'get', u'liberal']\n",
      "[u'thank', u'trump2016', u'great', u'makeamericagreatagain', u'people', u'amazing', u'trump', u'new', u'go', u'lets']\n",
      "[u'america', u'make', u'great', u'again', u'trump', u'true', u'rubio', u'deal', u'matter', u'beck']\n",
      "[u'border', u'fail', u'read', u'guy', u'sure', u'story', u'old', u'gun', u'change', u'trump']\n",
      "[u'great', u'keep', u'trump', u'people', u'veteran', u'carson', u'today', u'this', u'get', u'host']\n",
      "_____\n",
      "RealCandaceO\n",
      "[u'trump', u'good', u'need', u'know', u'love', u'donaldtrump', u'right', u'reagan', u'time', u'what']\n",
      "[u'like', u'politician', u'talk', u'run', u'trump', u'great', u'silent', u'president', u'tell', u'people']\n",
      "[u'money', u'illegal', u'give', u'win', u'help', u'like', u'need', u'all', u'know', u'trump']\n",
      "[u'border', u'fail', u'read', u'guy', u'sure', u'story', u'old', u'gun', u'change', u'trump']\n",
      "[u'great', u'keep', u'trump', u'people', u'veteran', u'carson', u'today', u'this', u'get', u'host']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for follower_ in [i[0] for i in recommended]:\n",
    "    count = 0 \n",
    "    print '_____'\n",
    "    print follower_\n",
    "\n",
    "    for topic_ in followers_topic_dict[follower_].values():\n",
    "        if count < 1:\n",
    "        \n",
    "            for trump_topic in trump_topics.values():\n",
    "\n",
    "\n",
    "                    if len(set(topic_).intersection(trump_topic)) == 2 :\n",
    "                            points_dict[follower_] +=1\n",
    "                            count +=1\n",
    "                            print trump_topic\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This Recommender uses LDA \n",
    "2. LDA does not seem to be the \n",
    "3. Running Time has to be improved since this cannot be done real time\n",
    "\n",
    "Next Steps:\n",
    "1. Try RMF \n",
    "2. Build An App\n",
    "3. Get Lager Amounts Of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datasci27]",
   "language": "python",
   "name": "conda-env-datasci27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
